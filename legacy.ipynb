{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "# Importations nécessaires\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.hands = self.mp_hands.Hands(\n",
    "            static_image_mode=True,\n",
    "            max_num_hands=1,\n",
    "            min_detection_confidence=0.5\n",
    "        )\n",
    "\n",
    "    def extract_landmarks(self, image):\n",
    "        # Convertir le tensor PyTorch en numpy array\n",
    "        image_np = image.squeeze().numpy()  # Enlever la dimension du canal\n",
    "        \n",
    "        # Normaliser entre 0-255 et convertir en uint8\n",
    "        image_np = (image_np * 255).astype(np.uint8)\n",
    "        \n",
    "        # Convertir en RGB en répétant le canal\n",
    "        image_rgb = np.stack((image_np,) * 3, axis=-1)\n",
    "        \n",
    "        # Traitement MediaPipe\n",
    "        results = self.hands.process(image_rgb)\n",
    "        \n",
    "        landmarks = []\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                for landmark in hand_landmarks.landmark:\n",
    "                    landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "        \n",
    "        # Si aucun landmark n'est détecté, retourner des zéros\n",
    "        if not landmarks:\n",
    "            landmarks = [0.0] * 63  # 21 points * 3 coordonnées\n",
    "            \n",
    "        return torch.tensor(landmarks, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Définition de la classe BriareoDataset\n",
    "class BriareoDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.sequences = []  # Au lieu de samples, on stocke des séquences\n",
    "        self.feature_extractor = HandFeatureExtractor()\n",
    "\n",
    "        \n",
    "        print(f\"Chargement du dataset depuis {root_dir}\")\n",
    "\n",
    "        for person_id in range(0, 26):\n",
    "            person_dir = f\"{str(person_id).zfill(3)}\"\n",
    "            person_path = os.path.join(root_dir, person_dir)\n",
    "            \n",
    "            if not os.path.exists(person_path):\n",
    "                continue\n",
    "                \n",
    "            for gesture_id in range(12):\n",
    "                gesture_dir = f\"g{str(gesture_id).zfill(2)}\"\n",
    "                gesture_path = os.path.join(person_path, gesture_dir)\n",
    "                \n",
    "                if not os.path.exists(gesture_path):\n",
    "                    print(f\"Skipping {gesture_path}\")\n",
    "                    continue\n",
    "                \n",
    "                for repetition_id in range(3):\n",
    "                    repetition_dir = f\"{str(repetition_id).zfill(2)}\"\n",
    "                    repetition_path = os.path.join(gesture_path, repetition_dir)\n",
    "                    \n",
    "                    if not os.path.exists(repetition_path):\n",
    "                        print(f\"Skipping {repetition_path}\")\n",
    "                        continue\n",
    "                    \n",
    "                    sequence = {\n",
    "                        'person_id': person_id,\n",
    "                        'gesture_id': gesture_id,\n",
    "                        'repetition_id': repetition_id,\n",
    "                        'frames': []\n",
    "                    }\n",
    "                    \n",
    "                    valid_sequence = True\n",
    "                    for frame_id in range(40):\n",
    "                        l_img_path = os.path.join(repetition_path, 'L', 'raw', f\"{str(frame_id).zfill(3)}_rl.png\")\n",
    "                        r_img_path = os.path.join(repetition_path, 'R', 'raw', f\"{str(frame_id).zfill(3)}_rr.png\")\n",
    "\n",
    "                        \n",
    "                        if os.path.exists(l_img_path) and os.path.exists(r_img_path):\n",
    "                            sequence['frames'].append({\n",
    "                                'frame_id': frame_id,\n",
    "                                'l_img_path': l_img_path,\n",
    "                                'r_img_path': r_img_path\n",
    "                            })\n",
    "                        else:\n",
    "                            valid_sequence = False\n",
    "                            print(f\"Séquence incomplète : {l_img_path} ou {r_img_path} manquant\")\n",
    "                            break\n",
    "                    \n",
    "                    if valid_sequence and len(sequence['frames']) ==  40:\n",
    "                        self.sequences.append(sequence)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "   \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        sequence_features = []\n",
    "        \n",
    "        for frame in sequence['frames']:\n",
    "            l_image = Image.open(frame['l_img_path']).convert('L')\n",
    "            r_image = Image.open(frame['r_img_path']).convert('L')\n",
    "\n",
    "            if self.transform:\n",
    "                l_image = self.transform(l_image)\n",
    "                r_image = self.transform(r_image)\n",
    "            \n",
    "            # Extraire les landmarks pour les deux mains\n",
    "            l_landmarks = self.feature_extractor.extract_landmarks(l_image)\n",
    "            r_landmarks = self.feature_extractor.extract_landmarks(r_image)\n",
    "            \n",
    "            # Concaténer les features des deux mains\n",
    "            frame_features = torch.cat([l_landmarks, r_landmarks])\n",
    "            sequence_features.append(frame_features)\n",
    "        \n",
    "        return {\n",
    "            'features': torch.stack(sequence_features),  # [40, 126]\n",
    "            'gesture_id': sequence['gesture_id']\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du dataset depuis leap_motion/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1740148995.460480 7149444 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1740148995.478338 7152903 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740148995.483799 7152904 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Définir les transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Créer l'instance du dataset\n",
    "dataset = BriareoDataset(root_dir='leap_motion/train', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1740148998.075193 7152908 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'features': tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.4644,  0.4240, -0.0042],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.4646,  0.4225, -0.0089],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.4636,  0.4215, -0.0075],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.4577,  0.4256, -0.0046],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.4584,  0.4265, -0.0079],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.4582,  0.4266, -0.0069]]), 'gesture_id': 0}\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GestureClassifier(nn.Module):\n",
    "    def __init__(self, input_size=126, hidden_size=256, num_layers=2, num_classes=12):\n",
    "        super(GestureClassifier, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,      # 126 features (63 landmarks par main)\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,           # Format: [batch, sequence, features]\n",
    "            dropout=0.3,\n",
    "            bidirectional=True          # LSTM bidirectionnel pour mieux capturer le contexte\n",
    "        )\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 64),  # *2 car bidirectionnel\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # Classification layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, 40, 126]\n",
    "        \n",
    "        # LSTM output\n",
    "        lstm_out, _ = self.lstm(x)  # [batch, 40, hidden_size*2]\n",
    "        \n",
    "        # Attention weights\n",
    "        attention_weights = self.attention(lstm_out)  # [batch, 40, 1]\n",
    "        \n",
    "        # Apply attention\n",
    "        context = torch.sum(attention_weights * lstm_out, dim=1)  # [batch, hidden_size*2]\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(context)  # [batch, num_classes]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm  # Changement ici\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=50):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Utilisation de : {device}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    history = {'train_acc': [], 'train_loss': [], 'val_acc': [], 'val_loss': []}\n",
    "    \n",
    "    # Barre de progression principale pour les epochs\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        # Mode entraînement\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        # Barre de progression pour les batches\n",
    "        train_loop = tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\", \n",
    "                         leave=False, position=1)\n",
    "        \n",
    "        for batch in train_loop:\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['gesture_id'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Mise à jour de la barre de progression des batches\n",
    "            train_loop.set_postfix(\n",
    "                loss=f\"{train_loss/train_total:.4f}\",\n",
    "                acc=f\"{100.*train_correct/train_total:.2f}%\"\n",
    "            )\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        val_loop = tqdm(val_loader, desc=f\"Val Epoch {epoch+1}\", \n",
    "                       leave=False, position=1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loop:\n",
    "                features = batch['features'].to(device)\n",
    "                labels = batch['gesture_id'].to(device)\n",
    "                \n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                # Mise à jour de la barre de validation\n",
    "                val_loop.set_postfix(\n",
    "                    loss=f\"{val_loss/val_total:.4f}\",\n",
    "                    acc=f\"{100.*val_correct/val_total:.2f}%\"\n",
    "                )\n",
    "        \n",
    "        # Calcul des métriques finales de l'epoch\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Mise à jour de l'historique\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        # Sauvegarde du meilleur modèle\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            \n",
    "        # Affichage des métriques de l'epoch\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "100%|██████████| 100/100 [00:00<00:00, 819200.00it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_x = range(100)\n",
    "train_y = range(200)\n",
    "\n",
    "train_iter = zip(train_x, train_y)\n",
    "\n",
    "# Notice `train_iter` can only be iter over once, so i get `total` in this way.\n",
    "total = min(len(train_x), len(train_y))\n",
    "\n",
    "with tqdm(total=total) as pbar:\n",
    "    for item in train_iter:\n",
    "        # do something ...\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, val_loader, criterion, device):\n",
    "    # Phase de validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['gesture_id'].to(device)\n",
    "            \n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    # Calcul des métriques de validation\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = 100. * val_correct / val_total\n",
    "\n",
    "    return avg_val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, train_loader, criterion, optimizer, device, progress_bar):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        features = batch['features'].to(device)\n",
    "        labels = batch['gesture_id'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "\n",
    "        progress_bar.write(\n",
    "            f\"Train Loss: {train_loss/train_total:.4f} | Train Acc: {100.*train_correct/train_total:.2f}%\"\n",
    "        )\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    # Calcul des métriques d'entraînement\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_acc = 100. * train_correct / train_total\n",
    "\n",
    "    return train_acc, avg_train_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm  # tqdm.auto s'adapte automatiquement à l'environnement\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=50):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Utilisation de : {device}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    best_val_acc = 0\n",
    "    history = {'train_acc': [], 'train_loss': [], 'val_acc': [], 'val_loss': []}\n",
    "    \n",
    "    print(\"Entraînement du modèle...\")\n",
    "    with tqdm(range(num_epochs), desc=\"Entrainement\") as pbar:\n",
    "        train_acc, avg_train_loss = train_batch(model, train_loader, criterion, optimizer, device, pbar)\n",
    "        avg_val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Mise à jour de l'historique\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        \n",
    "        # # Sauvegarde du meilleur modèle\n",
    "        # if val_acc > best_val_acc:\n",
    "        #     best_val_acc = val_acc\n",
    "        #     torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation de : cpu\n",
      "Entraînement du modèle...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7da650a2734b31bf5f02d341d06b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entrainement:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Entraînement du modèle\u001b[39;00m\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m GestureClassifier()\n\u001b[0;32m---> 13\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntraînement du modèle...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntrainement\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m---> 18\u001b[0m     train_acc, avg_train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     avg_val_loss, val_acc \u001b[38;5;241m=\u001b[39m evaluate_model(model, val_loader, criterion, device)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Mise à jour de l'historique\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m, in \u001b[0;36mtrain_batch\u001b[0;34m(model, train_loader, criterion, optimizer, device, progress_bar)\u001b[0m\n\u001b[1;32m      4\u001b[0m train_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m train_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgesture_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/AntoineMaes/HandGestureRecognition/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/Documents/GitHub/AntoineMaes/HandGestureRecognition/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Documents/GitHub/AntoineMaes/HandGestureRecognition/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/GitHub/AntoineMaes/HandGestureRecognition/.venv/lib/python3.12/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[8], line 81\u001b[0m, in \u001b[0;36mBriareoDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Extraire les landmarks pour les deux mains\u001b[39;00m\n\u001b[1;32m     80\u001b[0m l_landmarks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor\u001b[38;5;241m.\u001b[39mextract_landmarks(l_image)\n\u001b[0;32m---> 81\u001b[0m r_landmarks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_landmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Concaténer les features des deux mains\u001b[39;00m\n\u001b[1;32m     84\u001b[0m frame_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([l_landmarks, r_landmarks])\n",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m, in \u001b[0;36mHandFeatureExtractor.extract_landmarks\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     18\u001b[0m image_rgb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack((image_np,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Traitement MediaPipe\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m landmarks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n",
      "File \u001b[0;32m~/Documents/GitHub/AntoineMaes/HandGestureRecognition/.venv/lib/python3.12/site-packages/mediapipe/python/solutions/hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[1;32m    133\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/AntoineMaes/HandGestureRecognition/.venv/lib/python3.12/site-packages/mediapipe/python/solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[1;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Utilisation :\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512)\n",
    "\n",
    "# Entraînement du modèle\n",
    "model = GestureClassifier()\n",
    "history = train_model(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbafd51514734d229920f9103f3fce84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 0\n",
      "processed: 1\n",
      "processed: 2\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "values = range(3)\n",
    "with tqdm(total=len(values)) as pbar:\n",
    "    for i in values:\n",
    "        pbar.write('processed: %d' %i)\n",
    "        pbar.update(1)\n",
    "        sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.optim import Adam\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def train_model(model, train_loader, val_loader, num_epochs=50):\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     print(f\"Utilisation de : {device}\")\n",
    "    \n",
    "#     model = model.to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "    \n",
    "#     best_val_acc = 0\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Mode entraînement\n",
    "#         model.train()\n",
    "#         train_loss = 0\n",
    "#         train_correct = 0\n",
    "#         train_total = 0\n",
    "        \n",
    "#         for batch in train_loader:\n",
    "#             features = batch['features'].to(device)\n",
    "#             labels = batch['gesture_id'].to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(features)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             train_loss += loss.item()\n",
    "#             _, predicted = outputs.max(1)\n",
    "#             train_total += labels.size(0)\n",
    "#             train_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "#         # Mode validation\n",
    "#         model.eval()\n",
    "#         val_loss = 0\n",
    "#         val_correct = 0\n",
    "#         val_total = 0\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for batch in val_loader:\n",
    "#                 features = batch['features'].to(device)\n",
    "#                 labels = batch['gesture_id'].to(device)\n",
    "                \n",
    "#                 outputs = model(features)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "                \n",
    "#                 val_loss += loss.item()\n",
    "#                 _, predicted = outputs.max(1)\n",
    "#                 val_total += labels.size(0)\n",
    "#                 val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "#         # Calcul des métriques\n",
    "#         train_acc = 100. * train_correct / train_total\n",
    "#         val_acc = 100. * val_correct / val_total\n",
    "        \n",
    "#         print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "#         print(f'Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%')\n",
    "#         print(f'Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        \n",
    "#         # Mise à jour du scheduler\n",
    "#         scheduler.step(val_loss)\n",
    "        \n",
    "#         # Sauvegarde du meilleur modèle\n",
    "#         if val_acc > best_val_acc:\n",
    "#             best_val_acc = val_acc\n",
    "#             torch.save(model.state_dict(), 'best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm  # Changé pour une meilleure intégration avec Jupyter\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# import torch.nn as nn\n",
    "\n",
    "# def train_model(model, train_loader, val_loader, num_epochs=50):\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     print(f\"Utilisation de : {device}\")\n",
    "    \n",
    "#     model = model.to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "#     # Configuration de la barre de progression principale\n",
    "#     epoch_pbar = tqdm(range(num_epochs), desc='Training Progress', position=0)\n",
    "    \n",
    "#     best_val_acc = 0\n",
    "#     history = {'train_acc': [], 'train_loss': [], 'val_acc': [], 'val_loss': []}\n",
    "    \n",
    "#     for epoch in epoch_pbar:\n",
    "#         # Phase d'entraînement\n",
    "#         model.train()\n",
    "#         train_loss = 0\n",
    "#         train_correct = 0\n",
    "#         train_total = 0\n",
    "        \n",
    "#         # Désactiver les sous-barres de progression pour plus de clarté\n",
    "#         for batch in train_loader:\n",
    "#             try:\n",
    "#                 features = batch['features'].to(device)\n",
    "#                 labels = batch['gesture_id'].to(device)\n",
    "                \n",
    "#                 optimizer.zero_grad()\n",
    "#                 outputs = model(features)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "                \n",
    "#                 train_loss += loss.item()\n",
    "#                 _, predicted = outputs.max(1)\n",
    "#                 train_total += labels.size(0)\n",
    "#                 train_correct += predicted.eq(labels).sum().item()\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Erreur dans le batch d'entraînement : {str(e)}\")\n",
    "#                 continue\n",
    "        \n",
    "#         # Phase de validation\n",
    "#         model.eval()\n",
    "#         val_loss = 0\n",
    "#         val_correct = 0\n",
    "#         val_total = 0\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for batch in val_loader:\n",
    "#                 try:\n",
    "#                     features = batch['features'].to(device)\n",
    "#                     labels = batch['gesture_id'].to(device)\n",
    "                    \n",
    "#                     outputs = model(features)\n",
    "#                     loss = criterion(outputs, labels)\n",
    "                    \n",
    "#                     val_loss += loss.item()\n",
    "#                     _, predicted = outputs.max(1)\n",
    "#                     val_total += labels.size(0)\n",
    "#                     val_correct += predicted.eq(labels).sum().item()\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Erreur dans le batch de validation : {str(e)}\")\n",
    "#                     continue\n",
    "        \n",
    "#         # Calcul des métriques\n",
    "#         train_acc = 100. * train_correct / train_total if train_total > 0 else 0\n",
    "#         train_loss = train_loss / len(train_loader) if len(train_loader) > 0 else 0\n",
    "#         val_acc = 100. * val_correct / val_total if val_total > 0 else 0\n",
    "#         val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "        \n",
    "#         # Mise à jour de l'historique\n",
    "#         history['train_acc'].append(train_acc)\n",
    "#         history['train_loss'].append(train_loss)\n",
    "#         history['val_acc'].append(val_acc)\n",
    "#         history['val_loss'].append(val_loss)\n",
    "        \n",
    "#         # Mise à jour de la barre de progression\n",
    "#         epoch_pbar.set_postfix({\n",
    "#             'train_loss': f'{train_loss:.4f}',\n",
    "#             'train_acc': f'{train_acc:.2f}%',\n",
    "#             'val_loss': f'{val_loss:.4f}',\n",
    "#             'val_acc': f'{val_acc:.2f}%'\n",
    "#         })\n",
    "        \n",
    "#         # Sauvegarde du meilleur modèle\n",
    "#         if val_acc > best_val_acc:\n",
    "#             best_val_acc = val_acc\n",
    "#             torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "#     return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation de : cpu\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m model \u001b[38;5;241m=\u001b[39m GestureClassifier()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Lancement de l'entraînement\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 15\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Configuration de la barre de progression\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m pbar \u001b[38;5;241m=\u001b[39m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEntraînement\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m best_val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     18\u001b[0m history \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n",
      "File \u001b[0;32m~/Documents/GitHub/AntoineMaes/HandGestureRecognition/.venv/lib/python3.12/site-packages/tqdm/notebook.py:234\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m unit_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    233\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m*\u001b[39m unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mpbar \u001b[38;5;241m=\u001b[39m proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/AntoineMaes/HandGestureRecognition/.venv/lib/python3.12/site-packages/tqdm/notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[1;32m    110\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m IProgress(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mtotal)\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "# from torch.utils.data import random_split\n",
    "# # Préparation des données avec worker_init_fn\n",
    "# def worker_init_fn(worker_id):\n",
    "#     np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "# # Création des DataLoaders\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# val_size = len(dataset) - train_size\n",
    "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# # DataLoaders sans parallélisation pour le moment\n",
    "# train_loader = DataLoader(\n",
    "#     train_dataset, \n",
    "#     batch_size=32, \n",
    "#     shuffle=True,\n",
    "#     num_workers=0  # Changé de 4 à 0\n",
    "# )\n",
    "# val_loader = DataLoader(\n",
    "#     val_dataset, \n",
    "#     batch_size=32,\n",
    "#     num_workers=0  # Changé de 4 à 0\n",
    "# )\n",
    "\n",
    "# # Création du modèle\n",
    "# model = GestureClassifier()\n",
    "\n",
    "# # Lancement de l'entraînement\n",
    "# train_model(model, train_loader, val_loader, num_epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
